---
title: "【AWS S3】 Python boto3 を使い操作してみる"
emoji: "🔄"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [aws, S3, Python, boto3]
published: true
---

## はじめに
AWS 3S を Python `boto3` を使って Bucket 作成から操作してみました。
エラーハンドリング・ログ吐き出しなど荒いのでツッコミどころあるかと。
力不足なので、引き続き実装は継続して続けていきます!!

とりあえず、意図した動作は実装できたので流れを残しておこうかと。


## できたこと
- S3 バッケット:【生成 / 削除】
- S3 ファイル:【アップロード / 削除】


## 後に試したいこと

- ダウンロード機能・大量データの一括アップロード
- `response` の値を使って単体テストを書いてみる
- `mock` もやってみたい



## 動作環境

```bash
- Mac M1
- Python 3.8

# 追加パッケージ
- boto3
```

## リポジトリ
色々試しているリポジトリになるので、本記事で書いた動作以外も含まれています🙏

- [python_test_fixture](https://github.com/KazusaNakagawa/python_test_fixture)

## 前提条件

- AWS アカウントあり
- AWS IMA作成済


## 前処理
ローカルで、IMAユーザの設定が必要だったので、`AWS CLI` をダウンロードしました。
他にも、環境変数に持たせて IMAユーザを呼び出す方法がありそうでしたが、試してませんー


### AWS CLI のダウンロード [#1](https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2-mac.html)

* macOS pkg ファイルをダウンロード

  最新バージョンの **AWS CLI** をインストール


* シンボリックリンクを手動で作成 `PATH` を通す

```bash:bash
$ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws
$ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer
```


* インストールの検証

```bash:bash
$ which aws
/usr/local/bin/aws

$ aws --version
aws-cli/2.2.36 Python/3.8.8 Darwin/20.6.0 exe/x86_64 prompt/off
```


### [Configuration](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#configuration) の設定

AWS CLI をインストールしたので下記コマンドで以前作成していた**IAMユーザ**(作成方法は割愛)でaws にアクセスできるように設定します。


```bash:bash
# 実行コマンド
$ aws configure

# 聞かれる項目を入力
AWS Access Key ID [None]: <Access Key ID> # IAM の　key
AWS Secret Access Key [None]: <Secret Access Key> # IAM の　key
Default region name [None]: ap-northeast-1 # #2を参考にtokyoを選択
Default output format [None]: json # とりあえず json
```


## ツリー構造

- 以下のファイルで実装しました!
```bash:tree
├── api
│   ├── __init__.py
│   ├── models
│   │   ├── __init__.py
│   │   ├── bucket.py
│   │   ├── logger_tool.py
│   │   └── time.py
├── config
│   ├── __init__.py
│   └── const.py
├── data
│   ├── test.sql
│   ├── user.json
│   └── user.sql
├── log
│   └── bucket.log
├── main.py

```

## 作成ファイル

S3 Bucket を操作する model を作成しました。
基本 `Bucket model` に機能を追加していく予定です!!

今は、**Bucket 作成・削除**、**ファイルアップロード・削除**を実装しました!
ログ取得、例外処理が甘いと感じておりますが、とりあえず進めます。

`logger` が多くてみにくい...。

bucket 削除も一括じゃなくて、指定 bucket のみ削除するようにしていきたいと考えています。


```python:api/models/bucket.py
import boto3

from botocore.exceptions import ClientError

from api.models import logger_tool
from config import const


class Bucket(object):

    def __init__(self, client: str, bucket_name: str, region: str):
        """

        params
        ------
        client(str): s3
        bucket_name(str): choice bucket name
        region(str): Region code
        """
        self.client = boto3.client(client)
        self.resource_bucket = boto3.resource(client)
        self.bucket_name = bucket_name
        self.region = region

    def is_bucket_check(self, bucket_name: str, max_key: int):
        """
        作成するbucket が既に存在するかチェックする

        :return:
            bucketがある時はbucketを作成しない
        """
        # TODO:【暫定】例外でbucket有無の切り分ける方法は避けたい
        try:
            response = self.client.list_objects_v2(
                Bucket=bucket_name,
                MaxKeys=max_key,
            )
            return True

        except ClientError as ex:
            logger_tool.error(
                action='bucket check',
                status='error',
                bucket_name=bucket_name,
                ex=ex
            )
            return False

    def create_bucket(self):
        """ S3 に bucket を作成する """

        logger_tool.info(
            action='create',
            status='run',
            bucket_name=self.bucket_name
        )

        if not self.is_bucket_check(self.bucket_name, 2):
            self.client.create_bucket(
                Bucket=self.bucket_name,
                CreateBucketConfiguration={'LocationConstraint': self.region}
            )
            logger_tool.info(
                action='create',
                status=200,
                bucket_name=self.bucket_name
            )

    def upload_data(self, bucket_name: str, upload_data: str):
        """
        1 ファイルのみアップロードできる

        params
        ------
        upload_data: ex) sample.png

        """
        key = upload_data.split('/')[-1]

        logger_tool.info(
            action='upload',
            status='run',
            bucket_name=bucket_name,
            data=key,
        )

        with open(upload_data, 'rb') as upload_data_file:
            self.resource_bucket.Bucket(bucket_name).put_object(Key=key, Body=upload_data_file)

        logger_tool.info(
            action='upload',
            status=200,
            bucket_name=bucket_name,
            data=key,
        )

    def delete_data(self, bucket_name: str, delete_data: str):
        """
        bucketにあるデータを削除
        """

        key = delete_data.split('/')[-1]

        logger_tool.info(
            action='delete',
            status='run',
            bucket_name=bucket_name,
            data=key,
        )
        try:
            response = self.client.delete_object(
                Bucket=bucket_name,
                Key=key,
            )
            return response

        except ClientError as ex:
            logger_tool.error(
                action='delete data',
                status=404,
                bucket_name=bucket_name,
                ex=ex
            )

        logger_tool.info(
            action='delete',
            status=204,
            bucket_name=bucket_name,
            data=key,
        )

    def delete_all_buckets(self):
        """
        全ての bucketを削除する
        """
        
        buckets = list(self.resource_bucket.buckets.all())

        if buckets:
            logger_tool.info(
                action='delete',
                status='run',
                bucket_name=self.bucket_name
            )

            response = [key.delete() for key in self.resource_bucket.buckets.all()]

            logger_tool.info(
                action='delete',
                status=204,
                bucket_name=self.bucket_name
            )
            return response

```

### ログ吐き出しパラメータを設定

`{ key: value }` で出力したいパラメータを設定 

ログ吐き出しファイルの調整・行数をパラメータに入れてみたかったんですが割愛。
追々できればいいなっと考えています。


```python:api/models/logger_tool.py
import logging

from api.models import time
from config import const

from pathlib import Path

Path(const.LOGFILE_PATH).mkdir(exist_ok=True)
logging.basicConfig(filename=Path(f"{const.LOGFILE_PATH}{const.LOG_FILE}"), level=logging.INFO)
logger = logging.getLogger(__name__)


def error(action, status, bucket_name: str, ex):
    logger.error({
        'time': time.datetime_now(),
        'action': action,
        'status': status,
        'bucket name': bucket_name,
        'except': ex,
    })


def info(action, status, bucket_name: str, data=None):
    logger.info({
        'time': time.datetime_now(),
        'action': action,
        'status': status,
        'bucket name': bucket_name,
        'data': data,
    })


```

### ログ吐き出し時間を取り出すためのファイル

ログ吐き出し時間のフォーマットを調整。

```python:api/models/time.py
import datetime


def datetime_now():
    return datetime.datetime.now().isoformat()[:19]

```

```python:result
# 秒まで出力されるように調整
>>> '2021-09-12T23:42:32'
```

### const

定数としてまとめたかったので、一括にまとめた形です。
厳密には、呼び出した時に、書き換えできちゃうのであしからず。


```python:config/const.py
# 実行は 0: False, 1: True で切り分け
BUCKET_CREATE = 1
BUCKET_DELETE = 1

UPLOAD = 0
DELETE = 0

# ログ出力
LOGFILE_PATH = 'log/'
LOG_FILE = 'bucket.log'

# AWS
CLIENT = 's3'
BUCKET_NAME = 'testbucket'
TOKYO_REGION = 'ap-northeast-1'

```

### main

`main.py` で実行するように調整しました。
ほんまですと、バッチファイルを作成して定期実行のジョブを組むんでしょうがやった事ないので保留。それか、`lambad` で実行するんですかね🤔

また、次回以降試してみるのも面白そうですwww

```python:main.py
from api.models.bucket import Bucket
from config import const


def bucket_main():
    data_list = ['data/user.sql', 'data/user.json']

    # create instance
    bucket1 = Bucket(client=const.CLIENT, bucket_name=const.BUCKET_NAME, region=const.TOKYO_REGION)

    if const.BUCKET_CREATE:
        bucket1.create_bucket()

    if const.UPLOAD:
        for data in data_list:
            bucket1.upload_data(bucket_name=const.BUCKET_NAME, upload_data=data)

    if const.DELETE:
        for data in data_list:
            bucket1.delete_data(bucket_name=const.BUCKET_NAME, delete_data=data)

    if const.BUCKET_DELETE:
        bucket1.delete_all_buckets()


def main():
    bucket_main()


if __name__ == '__main__':
    main()

```



## 内容説明


### ログ吐き出しはこんな感じです

今回は `key: value` でログを吐き出すようにしてみました。


`api.models.logger_tool` が吐き出されているのがいやですね。
ここは書き出されるファイル名でログ吐き出しできるように調整したいです。

改善必須です。


```log:bucket.log
INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials
INFO:api.models.logger_tool:{'time': '2021-09-12T22:33:34', 'action': 'create', 'status': 'run', 'bucket name': 'testbucket-0911-2'}
ERROR:api.models.logger_tool:{'time': '2021-09-12T22:33:34', 'action': 'bucket check', 'status': 'error', 'bucket name': 'testbucket-0911-2', 'except': NoSuchBucket('An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist')}
INFO:api.models.logger_tool:{'time': '2021-09-12T22:33:36', 'action': 'create', 'status': 200, 'bucket name': 'testbucket-0911-2'}
INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials
INFO:api.models.logger_tool:{'time': '2021-09-12T22:33:58', 'action': 'create', 'status': 'run', 'bucket name': 'testbucket-0911-2'}
・・・
```


## まとめ
まだまだ、実用では使えないレベルですが、S3 の操作が軽く理解できたので良かったです。

次は、**ダウンロード機能**・**大量データの一括アップロード** など試してみたいですね。

私のアウトプットを込めた内容になっております。
もっとこう書いた方が良いなど、気軽にコメント頂けると幸いです🙏

ありがとうございました!!


## 参照


* [Boto3 Docs Quickstart](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html)
* [macOS での AWS CLI バージョン 2 のインストール、更新、アンインストール](https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2-mac.html) #1
* [認証情報の共有ファイルの作成](https://docs.aws.amazon.com/ja_jp/ses/latest/DeveloperGuide/create-shared-credentials-file.html)
* [AWS SDK for Python (Boto3)](https://aws.amazon.com/sdk-for-python/)
* [AWS 利用できるリージョン](https://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#:\~:text=%E3%83%AA%E3%83%BC%E3%82%B8%E3%83%A7%E3%83%B3%E3%81%AE%E6%8C%87%E5%AE%9A-,%E5%88%A9%E7%94%A8%E3%81%A7%E3%81%8D%E3%82%8B%E3%83%AA%E3%83%BC%E3%82%B8%E3%83%A7%E3%83%B3,-%E3%82%A2%E3%82%AB%E3%82%A6%E3%83%B3%E3%83%88%E3%81%AB%E3%82%88%E3%82%8A%E3%80%81%E5%88%A9%E7%94%A8) #2
* [Create an Amazon S3 bucket](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html#create-an-amazon-s3-bucket) 
* [https://github.com/aws/aws-cli/issues/2603#issuecomment-349191935](https://github.com/aws/aws-cli/issues/2603#issuecomment-349191935)