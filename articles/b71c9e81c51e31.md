---
title: "【AWS S3】 Python boto3 を使い操作してみる"
emoji: "🔄"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [aws, S3, Python, boto3]
published: true
---

## はじめに
AWS S3 を API `boto3` を使って Bucket 作成から操作してみました。
エラーハンドリング・ログ吐き出しなど荒いのでツッコミどころあるかと。
力不足なので、引き続き実装は継続して続けていきます!!

とりあえず、意図した動作は実装できたので流れを残しておこうかと。


## できたこと
- S3 バッケット:【生成 / 削除】
- S3 ファイル:【アップロード / ダウンロード /削除】

## 後に試したいこと

- ダウンロード・大量データの一括アップロード
- `response` の値を使って単体テストを書いてみる
- `mock` もやってみたい



## 動作環境

```python
- Mac M1
- Python: 3.9.1

# 追加パッケージ
- boto3: 1.18.44
```

:::message
boto3 は、[pypl ↗️](https://pypi.org/project/boto3/) の Getting Started　に従い 対象の repository を cloneするようです。
でないと、`$ python -m pip install boto3` で install できず Error しました。

> AttributeError: module 'pip._vendor_.six.moves.urllib.request' has no attribute 'addbase' 
:::

```bash:bash
# clone
$ git clone https://github.com/boto/boto3.git 
```


## リポジトリ
色々試しているリポジトリになるので、本記事で書いた動作以外も含まれています🙏

- [python_test_fixture](https://github.com/KazusaNakagawa/python_test_fixture)

## 前提条件

- AWS アカウントあり
- AWS IMA作成済


## 前処理
ローカルで、IMAユーザの設定が必要だったので、`AWS CLI` をダウンロードしました。
他にも、環境変数に持たせて IMAユーザを呼び出す方法がありそうでしたが、試してませんー


### AWS CLI のダウンロード [#1](https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2-mac.html)

* macOS pkg ファイルをダウンロード

  最新バージョンの **AWS CLI** をインストール


* シンボリックリンクを手動で作成 `PATH` を通す

```bash:bash
$ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws
$ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer
```


* インストールの検証

```bash:bash
$ which aws
/usr/local/bin/aws

$ aws --version
aws-cli/2.2.36 Python/3.8.8 Darwin/20.6.0 exe/x86_64 prompt/off
```


### [Configuration](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#configuration) の設定

AWS CLI をインストールしたので下記コマンドで以前作成していた**IAMユーザ**(作成方法は割愛)でaws にアクセスできるように設定します。


```bash:bash
# 実行コマンド
$ aws configure

# 聞かれる項目を入力
AWS Access Key ID [None]: <Access Key ID> # IAM の　key
AWS Secret Access Key [None]: <Secret Access Key> # IAM の　key
Default region name [None]: ap-northeast-1 # #2を参考にtokyoを選択
Default output format [None]: json # とりあえず json
```


## ツリー構成

- 以下のファイルで実装しました!
```bash:tree
├── api
│   ├── __init__.py
│   ├── controllers
│   │   ├── __init__.py
│   │   ├── bucket_controller.py # S3操作
│   ├── models
│   │   ├── __init__.py
│   │   ├── bucket.py # 機能実装
│   │   ├── logger_tool.py
│   │   └── time.py
├── config
│   ├── __init__.py
│   └── const.py # 定数まとめてる
├── data # s3 upload用
│   ├── response.json
│   ├── test.sql
│   ├── user.json
│   └── user.sql
├── log
│   └── bucket.log # S3操作出力ファイル
├── main.py # 実行ファイル
├── tmp # S3 dwonload 用
│   ├── user.json
│   └── user.sql
```

## 作成ファイル

### model

S3 Bucket を操作する model を作成しました。
基本 `Bucket model` に機能を追加していく予定です!!

今のところ、下記を実装しました!

- S3 バッケット:【 生成 / 削除 】
- S3 ファイル:【 単体: アップロード / ダウンロード / 削除 】

ログ取得、例外処理が甘いと感じておりますが、とりあえず進めます。

`logger` が多くてみにくい...。

bucket 削除も一括じゃなくて、指定 bucket のみ削除するようにしていきたいと考えています。


```python:api/models/bucket.py
import boto3

from botocore.exceptions import ClientError
from pathlib import Path

from api.models import logger_tool
from config import const


class Bucket(object):

    def __init__(self, client: str, bucket_name: str, region: str):
        """

        params
        ------
        client(str): s3
        bucket_name(str): choice bucket name
        region(str): Region code
        """
        self.client = boto3.client(client)
        self.resource_bucket = boto3.resource(client)
        self.bucket_name = bucket_name
        self.region = region

   def create_bucket(self):
        """ Bucket を作成する処理
        ref
        -------
        https://github.com/aws/aws-cli/issues/2603#issuecomment-349191935

        params
        -------
        max_key(int): default: 2
            is_bucket_check fuc args

        """

        logger_tool.info(
            action='create',
            status='run',
            bucket_name=self.bucket_name
        )

        try:
            self.client.create_bucket(
                Bucket=self.bucket_name,
                CreateBucketConfiguration={'LocationConstraint': self.region}
            )

        except self.client.exceptions.BucketAlreadyExists as err:
            logger_tool.error(
                action='create',
                status=409,
                bucket_name=self.bucket_name,
                ex=err,
                msg="Bucket {} already exists!".format(err.response['Error']['BucketName']),
            )
            raise err

        logger_tool.info(
            action='create',
            status=200,
            bucket_name=self.bucket_name
        )


    def upload_data(self, bucket_name: str, upload_data: str):
        """
        params
        ------
        upload_data: ex) sample.png
        """
        key = upload_data.split('/')[-1]

        logger_tool.info(
            action='upload',
            status='run',
            bucket_name=bucket_name,
            data=key,
        )

        with open(upload_data, 'rb') as upload_data_file:
            self.resource_bucket.Bucket(bucket_name).put_object(Key=key, Body=upload_data_file)

        logger_tool.info(
            action='upload',
            status=200,
            bucket_name=bucket_name,
            data=key,
        )

    def download_data(self, download_data: str) -> None:
        """
        Bucket にあるファイルをダウンロードする

        params
        ------
        download_data(str): S3にある指定データを download する
        """
        # download 用に作成
        Path(const.TMP_PATH).mkdir(exist_ok=True)

        logger_tool.info(
            action='download',
            status='run',
            bucket_name=self.bucket_name,
            data=download_data,
        )
        try:
            self.resource_bucket.meta.client.download_file(
                self.bucket_name, download_data, f"tmp/{download_data}"
            )
            logger_tool.info(
                action='download',
                status=204,
                bucket_name=self.bucket_name,
                data=download_data,
            )

        except ClientError as ex:
            logger_tool.error(
                action='download',
                status=404,
                bucket_name=self.bucket_name,
                ex=ex
            )

    def delete_data(self, bucket_name: str, delete_data: str):
        """
        bucketにあるデータを削除

        params
        ------
        bucket_name(str):  Bucket name
        delete_data(data): 削除するファイル名

        return
        ------
        response: 削除が成功した結果を返すレスポンス parameter
        """

        key = delete_data.split('/')[-1]

        logger_tool.info(
            action='delete',
            status='run',
            bucket_name=bucket_name,
            data=key,
        )
        try:
            response = self.client.delete_object(
                Bucket=bucket_name,
                Key=key,
            )
            return response

        except ClientError as ex:
            logger_tool.error(
                action='delete data',
                status=404,
                bucket_name=bucket_name,
                ex=ex
            )

        logger_tool.info(
            action='delete',
            status=204,
            bucket_name=bucket_name,
            data=key,
        )

        ...

    def delete_all_buckets(self):
        """
        全ての bucketを削除する

        return
        ------
        response: bucket を削除した結果を返す response parameter
        """

        buckets = list(self.resource_bucket.buckets.all())

        if buckets:
            logger_tool.info(
                action='delete',
                status='run',
                bucket_name=self.bucket_name
            )

            response = [key.delete() for key in self.resource_bucket.buckets.all()]

            logger_tool.info(
                action='delete',
                status=204,
                bucket_name=self.bucket_name
            )
            return response

```

### ログ吐き出しパラメータを設定

`{ key: value }` で出力したいパラメータを設定

ログ吐き出しファイルの調整・行数をパラメータに入れてみたかったんですが割愛。
追々できればいいなっと考えています。


```python:api/models/logger_tool.py
import logging

from api.models import time
from config import const

from pathlib import Path

# CREATE DIR
Path(const.LOGFILE_PATH).mkdir(exist_ok=True)

logging.basicConfig(filename=Path(f"{const.LOGFILE_PATH}{const.LOG_FILE}"), level=logging.INFO)
logger = logging.getLogger(__name__)


def error(action, status, bucket_name: str, ex, msg=None):
    logger.error({
        'time': time.datetime_now(),
        'action': action,
        'status': status,
        'bucket name': bucket_name,
        'except': ex,
        'msg': msg,
    })


def info(action, status, bucket_name: str, data=None):
    logger.info({
        'time': time.datetime_now(),
        'action': action,
        'status': status,
        'bucket name': bucket_name,
        'data': data,
    })

```

### ログ吐き出し時間を取り出すためのファイル

ログ吐き出し時間のフォーマットを調整。

```python:api/models/time.py
import datetime


def datetime_now():
    return datetime.datetime.now().isoformat()[:19]

```

```python:result
# 秒まで出力されるように調整
>>> '2021-09-12T23:42:32'
```

### const

定数としてまとめたかったので、一括にまとめた形です。
厳密には、呼び出した時に、書き換えできちゃうのであしからず。


```python:config/const.py
# 実行は 0: False, 1: True で切り分け

CLIENT = 's3'
BUCKET_NAME = 'testbucket-yyyymmdd'
TOKYO_REGION = 'ap-northeast-1'

LOGFILE_PATH = 'log/'
LOG_FILE = 'bucket.log'
TMP_PATH = 'tmp/'

BUCKET_CREATE = 1
BUCKET_DELETE = 1

UPLOAD = 1
DOWNLOAD = 0
DELETE = 1

```

因みに、上の設定でスクリプト実行しますと、

1. S3 にアクセス
2. バケットを作成
3. data/ 配下 のデータを アップロード
4. すぐに、S3 にアップロードした データを削除
5. バケットを削除


となります。

動きとしては、バケット作成して最後にすぐ消しています。
何やってんの ... っとなりますが、まあ動作確認としては良さそうです。


### controller

`bucket_controller.py` でS3操作をできるように調整しました。
ほんまですと、**バッチファイル**を作成して定期実行のジョブを組むんでしょうがやった事ないので保留。それか、**Lambad** で実行するんですかね🤔

また、次回以降試してみるのも面白そうですwww

```python:api/controllers/bucket_controller.py
from config import const
from api.models import bucket


def s3_bucket_management():
    """ aws s3 management """
    management_bucket = bucket.Bucket(client=const.CLIENT, bucket_name=const.BUCKET_NAME, region=const.TOKYO_REGION)

    # TODO: 指定 Directory のファイルを格納する
    data_list = ['user.sql', 'user.json']

    if const.BUCKET_CREATE:
        management_bucket.create_bucket()

    if const.UPLOAD:
        for data in data_list:
            management_bucket.upload_data(bucket_name=const.BUCKET_NAME, upload_data=f"data/{data}")

    if const.DOWNLOAD:
        for data in data_list:
            management_bucket.download_data(download_data=data)

    if const.DELETE:
        for data in data_list:
            management_bucket.delete_data(bucket_name=const.BUCKET_NAME, delete_data=f"data/{data}")

    if const.BUCKET_DELETE:
        management_bucket.delete_all_buckets()

    # 最終的に存在している Bucketを確認する
    management_bucket.print_bucket_name()


```

### main でスクリプト実行
最後に、実行処理を、`main.py` に書いておけば良さそうです。

```python:main.py
import api.controllers.bucket_controller

if __name__ == '__main__':
    api.controllers.bucket_controller.s3_bucket_management()
```



## 実行結果

`main.py` を実行し結果はこんな感じです!

```log:bucket.log
# IMA ユーザ で設定した認証情報を確認
INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials

# Bucketを生成
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:06', 'action': 'create', 'status': 'run', 'bucket name': 'testbucket-yyyymmdd', 'data': None}
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:08', 'action': 'create', 'status': 200, 'bucket name': 'testbucket-yyyymmdd', 'data': None}

# data/ 配下に格納していた指定データを S3 に upload
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:08', 'action': 'upload', 'status': 'run', 'bucket name': 'testbucket-yyyymmdd', 'data': 'user.sql'}
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:09', 'action': 'upload', 'status': 200, 'bucket name': 'testbucket-yyyymmdd', 'data': 'user.sql'}
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:09', 'action': 'upload', 'status': 'run', 'bucket name': 'testbucket-yyyymmdd', 'data': 'user.json'}
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:09', 'action': 'upload', 'status': 200, 'bucket name': 'testbucket-yyyymmdd', 'data': 'user.json'}

# そしてすぐさまデータ削除
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:09', 'action': 'delete', 'status': 'run', 'bucket name': 'testbucket-yyyymmdd', 'data': 'user.sql'}
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:09', 'action': 'delete', 'status': 'run', 'bucket name': 'testbucket-yyyymmdd', 'data': 'user.json'}

# んで、Bucket を削除
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:09', 'action': 'delete', 'status': 'run', 'bucket name': 'testbucket-yyyymmdd', 'data': None}
INFO:api.models.logger_tool:{'time': '2021-09-18T14:20:10', 'action': 'delete', 'status': 204, 'bucket name': 'testbucket-yyyymmdd', 'data': None}

```

中身をみていきます。

意図した流れで処理実行できているようです。

気になる点は、INFO:`api.models.logger_tool` とlogger_tool のファイル名でログが吐かれているところ!

ここは書き出されるファイル名でログ吐き出しできるように調整したいです。

改善必須です。




## まとめ
まだまだ、実用では使えないレベルですが、S3 の操作が軽く理解できたので良かったです。

次は、**大量データの一括アップロード** など試してみたいですね。

私のアウトプットを込めた内容になっております。
もっとこう書いた方が良いなど、気軽にコメント頂けると幸いです🙏

ありがとうございました!!


## 参考

- [Boto3 Docs Quickstart](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html)
- [Catching exceptions when using a resource client](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html#catching-exceptions-when-using-a-resource-client)
- https://pypi.org/project/boto3/
- [macOS での AWS CLI バージョン 2 のインストール、更新、アンインストール](https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2-mac.html) #1
- [認証情報の共有ファイルの作成](https://docs.aws.amazon.com/ja_jp/ses/latest/DeveloperGuide/create-shared-credentials-file.html)
- [AWS SDK for Python (Boto3)](https://aws.amazon.com/sdk-for-python/)
- [AWS 利用できるリージョン](https://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#:\~:text=%E3%83%AA%E3%83%BC%E3%82%B8%E3%83%A7%E3%83%B3%E3%81%AE%E6%8C%87%E5%AE%9A-,%E5%88%A9%E7%94%A8%E3%81%A7%E3%81%8D%E3%82%8B%E3%83%AA%E3%83%BC%E3%82%B8%E3%83%A7%E3%83%B3,-%E3%82%A2%E3%82%AB%E3%82%A6%E3%83%B3%E3%83%88%E3%81%AB%E3%82%88%E3%82%8A%E3%80%81%E5%88%A9%E7%94%A8) #2
- [Create an Amazon S3 bucket](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html#create-an-amazon-s3-bucket)
- [https://github.com/aws/aws-cli/issues/2603#issuecomment-349191935](https://github.com/aws/aws-cli/issues/2603#issuecomment-349191935)
- [HTTP response code for POST when resource already exists](https://stackoverflow.com/questions/3825990/http-response-code-for-post-when-resource-already-exists)
